# Query Caching

The FinRAG platform includes a built-in caching layer to reduce redundant LLM calls and significantly improve latency on repeated or similar queries.

## How It Works

### 1. Unique Hash Key Generation

Each query is cached under a **unique, deterministic key**:

- The cache key is generated by hashing a concatenated string of:
  ```
  query + user_id + group_id + role
  ```
- This ensures that even small changes in query text or user role result in distinct keys.
- The hashing algorithm used is typically `SHA-256` to ensure consistency and avoid collisions.

### 2. Cache Lookup and Short-Circuit

- On receiving a new query, the backend checks the cache store using the generated hash.
- If found, it short-circuits the vector search and LLM call, returning the stored result with a `{"cached": true}` flag.

```json
{
  "answer": "Apple’s Q2 revenue was $94.8B, up 2% YoY.",
  "model_name": "gpt-4",
  "tokens_input": 65,
  "tokens_output": 102,
  "retrieved_docs_count": 3,
  "cached": true
}
```

### 3. Caching New Results

If the result is **not found** in the cache:

- The system:
  1. Performs vector retrieval for group-specific documents
  2. Builds the group + role-aware prompt
  3. Calls the LLM for response generation
  4. Logs full metadata
  5. **Saves the answer to the cache**, using the generated hash as the key

### 4. Persistent Store and Future Scaling

- Current implementation uses either:
  - **In-memory Python dict** (dev mode)
  - **Disk-based JSON file** (e.g., `cache/user42.json`)
- Each record includes:
  - Answer
  - Model metadata
  - Token usage
  - Retrieval stats
  - Timestamps

#### Future improvements:
- Redis-based TTL cache for distributed deployments
- Automatic cache invalidation when a group’s document set is updated
- Analytics dashboard to track cache hit/miss ratios

---

### Example Cache Key Composition

```python
raw_key = f"{user_id}:{group_id}:{role}:{query.strip().lower()}"
hash_key = hashlib.sha256(raw_key.encode()).hexdigest()
```